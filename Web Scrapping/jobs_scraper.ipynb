{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T07:39:09.275008Z",
     "start_time": "2026-01-07T07:39:08.261415Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://www.myjobmag.co.ke/search/jobs\"\n",
    "QUERY = \"developer\"  # you can change this to other keywords like 'python', 'data', etc.\n",
    "OUTPUT_EXCEL = Path(\"myjobmag_developer_jobs.xlsx\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T07:39:10.243574Z",
     "start_time": "2026-01-07T07:39:09.677648Z"
    }
   },
   "source": [
    "def fetch_search_page(page: int = 1, query: str = QUERY) -> BeautifulSoup:\n",
    "    \"\"\"Fetch a single search results page and return its BeautifulSoup object.\"\"\"\n",
    "    params = {\"q\": query}\n",
    "    if page > 1:\n",
    "        params[\"page\"] = page\n",
    "\n",
    "    resp = requests.get(BASE_URL, params=params, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def parse_jobs_from_soup(soup: BeautifulSoup) -> list[dict]:\n",
    "    \"\"\"Parse job cards from a search results page into a list of dicts.\"\"\"\n",
    "    jobs = []\n",
    "\n",
    "    # Job cards appear as list items under the results section. We look for blocks with job titles like 'DevOps Developer at Davis & Shirtliff Group'.\n",
    "    results_container = soup.find(\"div\", class_=\"listings\") or soup  # fallback to whole soup if structure changes\n",
    "\n",
    "    # A robust pattern is to look for <h2> or <h3> elements that contain the job title and link.\n",
    "    for header in results_container.find_all([\"h2\", \"h3\"]):\n",
    "        a = header.find(\"a\")\n",
    "        if not a:\n",
    "            continue\n",
    "\n",
    "        title_text = a.get_text(strip=True)\n",
    "        job_url = a.get(\"href\")\n",
    "        if job_url and job_url.startswith(\"/\"):\n",
    "            job_url = f\"https://www.myjobmag.co.ke{job_url}\"\n",
    "\n",
    "        # Company name often appears near the header, for example the previous or parent element\n",
    "        company = None\n",
    "        parent = header.parent\n",
    "        if parent:\n",
    "            company_el = parent.find(\"a\", class_=\"company-name\") or parent.find(\"span\", class_=\"company\")\n",
    "            if company_el:\n",
    "                company = company_el.get_text(strip=True)\n",
    "\n",
    "        # Short description / snippet\n",
    "        summary = None\n",
    "        desc_el = parent.find(\"p\") if parent else None\n",
    "        if desc_el:\n",
    "            summary = desc_el.get_text(\" \", strip=True)\n",
    "\n",
    "        # Date – often shown in a small tag or span with the date text like '05 January'\n",
    "        date_posted = None\n",
    "        date_el = parent.find(\"span\", class_=\"job-date\") or parent.find(\"time\")\n",
    "        if date_el:\n",
    "            date_posted = date_el.get_text(strip=True)\n",
    "\n",
    "        jobs.append({\n",
    "            \"title\": title_text,\n",
    "            \"company\": company,\n",
    "            \"summary\": summary,\n",
    "            \"date_posted_text\": date_posted,\n",
    "            \"job_url\": job_url,\n",
    "        })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def scrape_all_pages(max_pages: int | None = None) -> list[dict]:\n",
    "    \"\"\"Scrape all result pages until there are no new jobs or max_pages is reached.\"\"\"\n",
    "    all_jobs: list[dict] = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        soup = fetch_search_page(page)\n",
    "        jobs = parse_jobs_from_soup(soup)\n",
    "\n",
    "        if not jobs:\n",
    "            print(\"No jobs found on this page. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_jobs.extend(jobs)\n",
    "\n",
    "        # Stop if there is no 'next' button or we reached max_pages\n",
    "        pagination = soup.find(\"ul\", class_=\"pagination\") or soup.find(\"div\", class_=\"pagination\")\n",
    "        has_next = pagination and (\"Next\" in pagination.get_text())\n",
    "\n",
    "        if max_pages is not None and page >= max_pages:\n",
    "            print(\"Reached max_pages limit. Stopping.\")\n",
    "            break\n",
    "        if not has_next:\n",
    "            print(\"No 'Next' button found. Assuming last page.\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return all_jobs\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T07:39:10.516453Z",
     "start_time": "2026-01-07T07:39:10.282521Z"
    }
   },
   "source": [
    "def normalize_jobs(jobs: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Convert list of job dicts into a clean DataFrame with extra metadata.\"\"\"\n",
    "    df = pd.DataFrame(jobs)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Add scrape metadata\n",
    "    df[\"scrape_timestamp\"] = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Basic cleanup\n",
    "    df[\"title\"] = df[\"title\"].astype(str).str.strip()\n",
    "    df[\"company\"] = df[\"company\"].astype(str).str.strip()\n",
    "    df[\"job_url\"] = df[\"job_url\"].astype(str).str.strip()\n",
    "\n",
    "    # Create a simple unique key for de-duplication\n",
    "    df[\"job_key\"] = (df[\"title\"] + \" | \" + df[\"company\"] + \" | \" + df[\"job_url\"]).str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_with_existing(df_new: pd.DataFrame, output_path: Path = OUTPUT_EXCEL) -> pd.DataFrame:\n",
    "    \"\"\"Merge new scraped data with existing Excel file, avoiding duplicates.\"\"\"\n",
    "    if output_path.exists():\n",
    "        existing = pd.read_excel(output_path)\n",
    "        if \"job_key\" not in existing.columns:\n",
    "            # Rebuild job_key if missing\n",
    "            existing[\"title\"] = existing[\"title\"].astype(str)\n",
    "            existing[\"company\"] = existing[\"company\"].astype(str)\n",
    "            existing[\"job_url\"] = existing[\"job_url\"].astype(str)\n",
    "            existing[\"job_key\"] = (existing[\"title\"] + \" | \" + existing[\"company\"] + \" | \" + existing[\"job_url\"]).str.lower()\n",
    "        combined = pd.concat([existing, df_new], ignore_index=True)\n",
    "        combined = combined.drop_duplicates(subset=[\"job_key\"]).reset_index(drop=True)\n",
    "        return combined\n",
    "    else:\n",
    "        return df_new\n",
    "\n",
    "\n",
    "def save_to_excel(df: pd.DataFrame, output_path: Path = OUTPUT_EXCEL) -> None:\n",
    "    \"\"\"Save the DataFrame to an Excel file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Saved {len(df)} jobs to {output_path}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T07:39:47.789574Z",
     "start_time": "2026-01-07T07:39:10.703564Z"
    }
   },
   "source": [
    "# Run the full scraping pipeline\n",
    "\n",
    "jobs_raw = scrape_all_pages()  # you can pass max_pages=2 for quick tests\n",
    "print(f\"Scraped {len(jobs_raw)} raw jobs.\")\n",
    "\n",
    "jobs_df = normalize_jobs(jobs_raw)\n",
    "print(f\"After normalization: {len(jobs_df)} jobs.\")\n",
    "\n",
    "merged_df = merge_with_existing(jobs_df)\n",
    "print(f\"After merging with existing Excel: {len(merged_df)} unique jobs.\")\n",
    "\n",
    "save_to_excel(merged_df)\n",
    "\n",
    "merged_df.head()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "No 'Next' button found. Assuming last page.\n",
      "Scraped 18 raw jobs.\n",
      "After normalization: 18 jobs.\n",
      "After merging with existing Excel: 18 unique jobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkyalo\\AppData\\Local\\Temp\\ipykernel_80176\\2656231451.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  df[\"scrape_timestamp\"] = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 18 jobs to myjobmag_developer_jobs.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               title company summary  \\\n",
       "0        DevOps Developer at Davis & Shirtliff Group    None    None   \n",
       "1  Software Developer Attaché (3 Months Contract)...    None    None   \n",
       "2  Software Developer - 6 Posts at University of ...    None    None   \n",
       "3         Account Developer at The Coca-Cola Company    None    None   \n",
       "4  ICT Senior Officer – Data Architect and Develo...    None    None   \n",
       "\n",
       "  date_posted_text                                            job_url  \\\n",
       "0             None  https://www.myjobmag.co.ke/job/devops-develope...   \n",
       "1             None  https://www.myjobmag.co.ke/job/software-develo...   \n",
       "2             None  https://www.myjobmag.co.ke/job/software-develo...   \n",
       "3             None  https://www.myjobmag.co.ke/job/account-develop...   \n",
       "4             None  https://www.myjobmag.co.ke/job/ict-senior-offi...   \n",
       "\n",
       "      scrape_timestamp                                            job_key  \n",
       "0  2026-01-07 07:39:20  devops developer at davis & shirtliff group | ...  \n",
       "1  2026-01-07 07:39:20  software developer attaché (3 months contract)...  \n",
       "2  2026-01-07 07:39:20  software developer - 6 posts at university of ...  \n",
       "3  2026-01-07 07:39:20  account developer at the coca-cola company | n...  \n",
       "4  2026-01-07 07:39:20  ict senior officer – data architect and develo...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>summary</th>\n",
       "      <th>date_posted_text</th>\n",
       "      <th>job_url</th>\n",
       "      <th>scrape_timestamp</th>\n",
       "      <th>job_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DevOps Developer at Davis &amp; Shirtliff Group</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.myjobmag.co.ke/job/devops-develope...</td>\n",
       "      <td>2026-01-07 07:39:20</td>\n",
       "      <td>devops developer at davis &amp; shirtliff group | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software Developer Attaché (3 Months Contract)...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.myjobmag.co.ke/job/software-develo...</td>\n",
       "      <td>2026-01-07 07:39:20</td>\n",
       "      <td>software developer attaché (3 months contract)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Software Developer - 6 Posts at University of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.myjobmag.co.ke/job/software-develo...</td>\n",
       "      <td>2026-01-07 07:39:20</td>\n",
       "      <td>software developer - 6 posts at university of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Developer at The Coca-Cola Company</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.myjobmag.co.ke/job/account-develop...</td>\n",
       "      <td>2026-01-07 07:39:20</td>\n",
       "      <td>account developer at the coca-cola company | n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICT Senior Officer – Data Architect and Develo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.myjobmag.co.ke/job/ict-senior-offi...</td>\n",
       "      <td>2026-01-07 07:39:20</td>\n",
       "      <td>ict senior officer – data architect and develo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this notebook every 24 hours\n",
    "\n",
    "1. Open this notebook in Jupyter (or VS Code / Cursor) and run all cells. This will:\n",
    "   - Scrape all `developer` jobs from [`https://www.myjobmag.co.ke/search/jobs?q=developer`](https://www.myjobmag.co.ke/search/jobs?q=developer).\n",
    "   - Merge with the existing `myjobmag_developer_jobs.xlsx` file (if it exists).\n",
    "   - Save an updated Excel file with all unique jobs.\n",
    "\n",
    "2. To automate it every 24 hours on Windows (simple approach):\n",
    "   - Install Jupyter and ensure `python` is in your PATH.\n",
    "   - Create a `.bat` file that runs:\n",
    "\n",
    "   ```bat\n",
    "   cd /d C:\\Users\\dkyalo\\Desktop\\Trial\n",
    "   jupyter nbconvert --to notebook --execute jobs_scraper.ipynb --output jobs_scraper_executed.ipynb\n",
    "   ```\n",
    "\n",
    "   - Use **Task Scheduler** to create a basic task that runs this `.bat` file once every day.\n",
    "\n",
    "3. Each daily run will append new jobs (if any) and avoid duplicates using the `job_key` column.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
